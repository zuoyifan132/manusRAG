"""
@File   : flash_browser.py
@Time   : 2025/04/15 16:19
@Author : Wind.FlashC
@Desc   : None
"""

import os
import sys
sys.path.append("..")

import json
import time
from datetime import datetime
import streamlit as st
from components.sidebar import FlashSidebar
from components.faq import truncate_markdown_table
from core.api_deepseek import LLMCaller
from core.flash_agent import FlashAgentPlanner
# from core.flash_rag import flash_rag_pipeline
from utils.tool_util import TOOLS, TOOLS_DESC


# Streamlit UI
# è®¾ç½®initial_sidebar_state ä¸º "collapsed"ï¼Œé»˜è®¤æŠ˜å ä¾§è¾¹æ æé«˜åˆå§‹åŠ è½½é€Ÿåº¦
st.set_page_config(page_title="Flash-Browser", page_icon="âš¡", layout="wide")
st.title("âš¡ Flash-Browser")
st.caption("ğŸš€ Powered by FlashC Group")

# >>> ä¾§è¾¹æ è®¾ç½®
Flashsidebar = FlashSidebar()
Flashsidebar.sidebar()

# >>> èŠå¤©çª—å£è®¾ç½®
uploaded_file = st.file_uploader(
    label="Upload a pdf, docx, or txt file",
    type=["pdf", "docx", "txt"],
    help="Scanned documents are not supported yet!",
    label_visibility="collapsed"
)
if uploaded_file:
    # æ¿€æ´»æ–‡æ¡£è§£ææ¨¡å¼
    try:
        file = flash_rag_pipeline(uploaded_file)
    except:
        pass

with st.expander("é«˜çº§åŠŸèƒ½"):
    agent_flag = st.checkbox("Flash Agent Mode")
    max_iterations = st.number_input(
        label="Max Iterations",
        min_value=1, max_value=15, value=15,
        help="è®¾ç½® Agent æœ€å¤§è¿­ä»£æ¬¡æ•°"
    )

# >>> å…¬å…±å‚æ•°æå–
system_prompt = Flashsidebar.system_prompt
selected_model = Flashsidebar.selected_model
temperature = Flashsidebar.temperature
max_tokens = Flashsidebar.max_tokens


@st.cache_resource
def get_model_caller(model_name):
    return LLMCaller(api_key="", model=model_name)


@st.cache_resource
def get_flash_agent_planner():
    """"""
    return FlashAgentPlanner()


caller = get_model_caller(selected_model)
flash_agent_planner = get_flash_agent_planner()


def stream(text: str, delay: float = 0.01):
    for char in text:
        yield char
        time.sleep(delay)


def flash_agent_workflow(question: str):
    """"""
    if not question:
        return

    num_iteration = 0   # ç”¨äºè®°å½•Agentè§„åˆ’è¿­ä»£æ¬¡æ•°
    is_cmd_exist = True  # ç”¨äºè®°å½•å½“å‰æ˜¯å¦å­˜åœ¨å·¥å…·è°ƒç”¨
    resp = {"content": "", "cmdInfo": []}   # ç”¨äºè®°å½•æœ€ç»ˆç­”æ¡ˆ

    # åˆå§‹åŒ–ä¼šè¯å†å²è®°å½•
    if "agent_history" not in st.session_state:
        st.session_state.agent_history = []

    #
    body = {
        "appName": "admin",
        "userId": "0",
        "sessionId": "0",
        "token": "0",
        "current": {
            "user": {
                "query": "",
                "info": {
                    "model": "DeepSeek-V3",
                    "datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "toolDesc": TOOLS_DESC,
                }
            },
            "response": []
        },
        "history": st.session_state.agent_history.copy()
    }

    # é—®é¢˜
    body["current"]["user"]["query"] = question

    st.session_state.messages.append({"role": "user", "content": question})

    with st.chat_message("human"):
        st.write(question)

    # Agent è§„åˆ’
    chat_content = ""
    with st.chat_message("ai"):

        content = f"ğŸ¤– æ­£åœ¨è§„åˆ’ä¸­ ..."
        chat_content += content
        st.write_stream(stream(content))

        while is_cmd_exist and num_iteration < max_iterations:
            # æ‰§è¡Œagentè§„åˆ’æœåŠ¡
            resp = flash_agent_planner.plan(body)
            # æ›´æ–°æ˜¯å¦å­˜åœ¨å·¥å…·è°ƒç”¨ä¿¡å·
            is_cmd_exist = bool(len(resp["cmdInfo"]) > 0)
            content = resp["content"]
            chat_content += content
            

            if is_cmd_exist:
                st.write_stream(stream(content)) 
                # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
                tool_names = [cmd_info["name"] for cmd_info in resp["cmdInfo"]]
                cmd_info_list = [{"name": cmd_info["name"], "args": cmd_info["args"]} for cmd_info in resp["cmdInfo"]]

                content = f"ğŸ”§ è°ƒç”¨å·¥å…·: {'ã€'.join(tool_names)}"
                chat_content += content
                st.write_stream(stream(content))
                content = f"ğŸ”§ è°ƒç”¨å‚æ•°: {json.dumps(cmd_info_list, indent=4, ensure_ascii=False)}"
                chat_content += content
                st.write_stream(stream(content))

                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                for idx, cmd_info in enumerate(cmd_info_list):
                    tool_name, tool_args = cmd_info["name"], cmd_info["args"]
                    try:
                        tool_res = TOOLS[tool_name](**tool_args)
                        content = f"âœ… å·¥å…·è°ƒç”¨æˆåŠŸ"
                        chat_content += content
                        st.write_stream(stream(content))

                        # åœ¨UIä¸­ä½¿ç”¨æŠ˜å åŒºåŸŸæ˜¾ç¤ºå®Œæ•´ç»“æœ
                        with st.expander(f"æŸ¥çœ‹å·¥å…· '{tool_name}' è¿”å›ç»“æœ", expanded=False):
                            st.markdown(tool_res)
                        
                        # ä¸ºchat_contentæ·»åŠ å¯èƒ½æˆªæ–­çš„ç»“æœ
                        is_table, truncated_res = truncate_markdown_table(tool_res, head_rows=15, tail_rows=5, truncation_message='... è¡¨æ ¼ä¸­é—´è¡Œå·²çœç•¥ï¼Œè¿™é‡Œä¸åšå®Œæ•´å±•ç¤º ...')
                        if is_table and truncated_res != tool_res:
                            content = f"Observation: {truncated_res}"
                        else:
                            content = f"Observation: {tool_res}"
                        chat_content += content


                    except Exception as exc:
                        tool_res = "å·¥å…·æ‰§è¡Œå¤±è´¥"
                        content = f"\nâŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {str(exc)}"
                        chat_content += content
                        st.write(content)
                    # æ›´æ–°å·¥å…·è°ƒç”¨ç»“æœ
                    resp["cmdInfo"][idx]["res"] = tool_res

            # æ›´æ–°ç”¨æˆ·è¾“å…¥ï¼Œå‡†å¤‡ä¸‹ä¸€è½®è§„åˆ’
            body["current"]["response"].append(resp)

            # æ›´æ–°Agentè§„åˆ’è¿­ä»£æ¬¡æ•°
            num_iteration += 1

        if num_iteration >= max_iterations:
            content = "-" * 3
            chat_content += content
            st.write(content)
            content = f"â—â— è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°!"
            chat_content += content
            st.write_stream(stream(content))
            body["current"]["response"].append({"content": "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼å¼ºåˆ¶è¿›è¡Œæ€»ç»“ï¼", "cmdInfo": []})
            resp = flash_agent_planner.plan(body)

        content = "-" * 3
        chat_content += content
        st.write(content)

        content = resp["content"]
        chat_content += content
        st.write_stream(stream(content))

        content = "-" * 3
        chat_content += content
        st.write(content)

    # å°†å®Œæ•´å“åº”æ·»åŠ åˆ°ä¼šè¯å†å²
    st.session_state.messages.append({"role": "assistant", "content": chat_content})
    
    current_copy = body['current'].copy()
    body['history'].append(current_copy)
    st.session_state.agent_history = body['history']  # æ›´æ–°session_stateä¸­çš„å†å²


def flash_llm_chat(question: str):
    """"""
    st.session_state.messages.append({"role": "user", "content": question})

    with st.chat_message("user"):
        st.write(question)

    # åˆ›å»ºå ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        # è°ƒç”¨æµå¼ç”Ÿæˆå‡½æ•°å¹¶æµå¼æ˜¾ç¤ºç»“æœ
        try:
            # æµå¼ç”Ÿæˆå¹¶æ˜¾ç¤º
            caller.model = selected_model
            for response_chunk in caller.chat_stream(
                messages=st.session_state.messages,
                temperature=temperature,
                max_tokens=max_tokens
            ):
                full_response += response_chunk
                message_placeholder.markdown(full_response + "â–Œ")  # æ¨¡æ‹Ÿå…‰æ ‡

            # å®Œæˆåæ˜¾ç¤ºå®Œæ•´å“åº”
            message_placeholder.markdown(full_response)
        except Exception as e:
            st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
            full_response = f"å¾ˆæŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"
            message_placeholder.markdown(full_response)

    # å°†å®Œæ•´å“åº”æ·»åŠ åˆ°ä¼šè¯å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})


# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "system", "content": system_prompt}]

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])


# ç”¨æˆ·è¾“å…¥
if question := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    if not agent_flag:
        flash_llm_chat(question)
    else:
        flash_agent_workflow(question)
