"""
@File   : flash_browser.py
@Time   : 2025/04/15 16:19
@Author : Wind.FlashC
@Desc   : None
"""

import sys
sys.path.append("..")

import json
import time
from datetime import datetime
import streamlit as st
from components.sidebar import FlashSidebar
from components.faq import truncate_markdown_table
from core.api_deepseek import LLMCaller
# from core.flash_agent import FlashAgentPlanner
from utils.tool_util import TOOLS, TOOLS_DESC
from manus.manus_deep_search_agent import DeepSearch
from manus.llm import OpenAILLM
from utils.aigc_api import openai_stream_generate
from openai import OpenAI
from services.config import OPENAI_API_KEY
from collections import defaultdict
import threading
import io
import queue
from contextlib import redirect_stdout


# Streamlit UI
# è®¾ç½®initial_sidebar_state ä¸º "collapsed"ï¼Œé»˜è®¤æŠ˜å ä¾§è¾¹æ æé«˜åˆå§‹åŠ è½½é€Ÿåº¦
st.set_page_config(page_title="ManusRAG", page_icon="âš¡", layout="wide")
st.title("âš¡ ManusRAG")
st.caption("ğŸš€ Powered by Evan ZUO")

# >>> ä¾§è¾¹æ è®¾ç½®
Flashsidebar = FlashSidebar()
Flashsidebar.sidebar()

with st.expander("é«˜çº§åŠŸèƒ½"):
    deep_search_flag = st.checkbox("DeepSearch Mode")
    max_iterations = st.number_input(
        label="Max Iterations",
        min_value=1, max_value=15, value=3,
        help="è®¾ç½® DeepSearch æœ€å¤§è¿­ä»£æ¬¡æ•°"
    )

# >>> å…¬å…±å‚æ•°æå–
system_prompt = Flashsidebar.system_prompt
selected_model = Flashsidebar.selected_model
temperature = Flashsidebar.temperature
max_tokens = Flashsidebar.max_tokens


@st.cache_resource
def get_model_caller(model_name):
    return LLMCaller(api_key="", model=model_name)


@st.cache_resource(show_spinner=False)
def get_deep_search_agent(model_name, max_iter):
    """è·å–DeepSearchä»£ç†ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å’Œæœ€å¤§è¿­ä»£æ¬¡æ•°"""
    llm = OpenAILLM(
        model=model_name,
        temperature=temperature,
        max_tokens=max_tokens
    )
    return DeepSearch(llm=llm, max_iter=max_iter)


caller = get_model_caller(selected_model)
# deep_search_agentçš„å®ä¾‹åŒ–ä¼šåœ¨è°ƒç”¨æ—¶è¿›è¡Œï¼Œä»¥ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„å‚æ•°


def stream(text: str, delay: float = 0.01):
    for char in text:
        yield char
        time.sleep(delay)


def format_retrieved_docs(retrieved_docs):
    """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ŒæŒ‰æ–‡æ¡£åˆ†ç»„å¹¶æ’åº"""
    if not retrieved_docs:
        return []
    
    # å°†æ£€ç´¢ç»“æœè½¬æ¢ä¸ºæ›´æ˜“äºå¤„ç†çš„æ ¼å¼
    formatted_results = []
    for i, doc in enumerate(retrieved_docs):
        formatted_results.append({
            "æ–‡æ¡£": f"æ–‡æ¡£ #{i+1}",
            "å†…å®¹": doc,
            "ç›¸å…³åº¦": 1.0  # ç”±äºæ²¡æœ‰ç›¸å…³åº¦ä¿¡æ¯ï¼Œé»˜è®¤è®¾ä¸º1.0
        })
    
    # æŒ‰æ–‡æ¡£åˆ†ç»„
    grouped_results = defaultdict(list)
    for result in formatted_results:
        grouped_results[result["æ–‡æ¡£"]].append(result)
    
    # è¿”å›åˆ†ç»„åçš„ç»“æœ
    return grouped_results


class ThreadSafeStringIO(io.StringIO):
    """çº¿ç¨‹å®‰å…¨çš„StringIOï¼Œç”¨äºåœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­æ•è·è¾“å‡º"""
    def __init__(self):
        super().__init__()
        self.lock = threading.Lock()
        self.output_queue = queue.Queue()
    
    def write(self, s):
        with self.lock:
            result = super().write(s)
            if s.strip():  # åªåŠ å…¥éç©ºç™½å­—ç¬¦
                # ç¡®ä¿æ¯ä¸ªè¾“å‡ºéƒ½æœ‰æ¢è¡Œç¬¦
                if not s.endswith('\n'):
                    s = s + '\n'
                self.output_queue.put(s)
            return result


def flash_deep_search_workflow(question: str):
    """DeepSearchå·¥ä½œæµï¼Œå®æ—¶æµå¼æ˜¾ç¤ºDeepSearchçš„æŸ¥è¯¢è¿‡ç¨‹"""
    if not question:
        return

    st.session_state.messages.append({"role": "user", "content": question})

    with st.chat_message("human"):
        st.write(question)

    # åˆ›å»ºç”¨äºæµå¼è¾“å‡ºçš„èŠå¤©æ¶ˆæ¯
    with st.chat_message("ai"):
        # åˆ›å»ºä¸€ä¸ªç”¨äºæ˜¾ç¤ºçŠ¶æ€çš„å ä½ç¬¦
        status_placeholder = st.empty()
        # åˆ›å»ºä¸€ä¸ªç”¨äºæ˜¾ç¤ºè¿‡ç¨‹çš„å ä½ç¬¦
        process_placeholder = st.empty()
        # åˆ›å»ºä¸€ä¸ªç”¨äºæ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆçš„å ä½ç¬¦
        message_placeholder = st.empty()
        # åˆ›å»ºä¸€ä¸ªç”¨äºæ˜¾ç¤ºå¬å›ç»“æœçš„å ä½ç¬¦
        retrieval_placeholder = st.empty()
        
        # æ˜¾ç¤ºåˆå§‹çŠ¶æ€
        status_placeholder.write("ğŸ” æ­£åœ¨æ·±åº¦æœç´¢ä¸­...")
        process_content = ""
        
        # åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„StringIOç”¨äºæ•è·è¾“å‡º
        output_stream = ThreadSafeStringIO()
        
        # è·å–DeepSearchå®ä¾‹
        deep_search_agent = get_deep_search_agent(selected_model, max_iterations)
        
        # å­˜å‚¨æœ€ç»ˆç»“æœçš„å˜é‡
        final_results = {"answer": "", "docs": []}
        
        # å®šä¹‰æ‰§è¡ŒæŸ¥è¯¢çš„å‡½æ•°
        def run_query():
            with redirect_stdout(output_stream):
                final_answer, retrieved_docs = deep_search_agent.query(query=question)
                final_results["answer"] = final_answer
                final_results["docs"] = retrieved_docs
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡ŒæŸ¥è¯¢
        query_thread = threading.Thread(target=run_query)
        query_thread.start()
        
        # æ˜¾ç¤ºè¿‡ç¨‹ä¸­çš„è¾“å‡º
        try:
            # æ˜¾ç¤ºæ­£åœ¨å¤„ç†çš„æ¶ˆæ¯
            process_content += f"ä½¿ç”¨æ¨¡å‹: {selected_model}, æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}\n\n"
            process_placeholder.code(process_content)
            
            # ä¸æ–­ä»é˜Ÿåˆ—ä¸­è·å–è¾“å‡ºå¹¶æ˜¾ç¤º
            while query_thread.is_alive() or not output_stream.output_queue.empty():
                try:
                    new_output = output_stream.output_queue.get(block=True, timeout=0.1)
                    process_content += new_output
                    process_placeholder.code(process_content)
                except queue.Empty:
                    pass
                
                # ç»™UIä¸€äº›æ—¶é—´æ¥æ›´æ–°
                time.sleep(0.01)
            
            # æ˜¾ç¤ºè¿‡ç¨‹å®Œæˆ
            process_content += "\nå¤„ç†å®Œæˆï¼\n"
            process_placeholder.code(process_content)
            
            # ç­‰å¾…çº¿ç¨‹å®Œæˆ
            query_thread.join()
            
            # æ›´æ–°çŠ¶æ€
            status_placeholder.write("ğŸ” æ£€ç´¢å®Œæˆï¼æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
            
            # æ·»åŠ æœ€ç»ˆç­”æ¡ˆçš„åˆ†éš”ç¬¦
            process_content += "\n" + "=" * 40 + "\nâœ… æœ€ç»ˆç­”æ¡ˆ:\n" + "=" * 40 + "\n\n"
            process_placeholder.code(process_content)
            
            # æµå¼æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆ
            response = ""
            for char in final_results["answer"]:
                response += char
                message_placeholder.markdown(response + "â–Œ")  # æ¨¡æ‹Ÿå…‰æ ‡
                time.sleep(0.005)  # å‡å°‘å»¶è¿Ÿï¼ŒåŠ å¿«æ˜¾ç¤ºé€Ÿåº¦
            
            # æœ€ç»ˆæ˜¾ç¤ºå®Œæ•´ç­”æ¡ˆ
            message_placeholder.markdown(response)
            
            # æ›´æ–°çŠ¶æ€
            status_placeholder.write("âœ… å›ç­”å®Œæˆï¼å¯æŸ¥çœ‹ä¸‹æ–¹æ£€ç´¢ç»“æœ")
            
            # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£
            grouped_results = format_retrieved_docs(final_results["docs"])
            
            # ä½¿ç”¨expanderæ˜¾ç¤ºæ£€ç´¢è¯¦ç»†ç»“æœï¼ˆæ”¾åœ¨ç­”æ¡ˆä¸‹æ–¹ï¼‰
            with retrieval_placeholder.container():
                with st.expander("ğŸ“š æŸ¥çœ‹å¬å›è¯¦ç»†ç»“æœ", expanded=False):
                    if grouped_results:
                        sorted_docs = list(grouped_results.keys())
                        for doc_idx, doc in enumerate(sorted_docs):
                            results_for_doc = grouped_results[doc]
                            st.markdown(f"### ğŸ“„ {doc}")
                            for i, result in enumerate(results_for_doc):
                                st.markdown(f"**ç‰‡æ®µ {i+1}**")
                                st.markdown(f"```\n{result['å†…å®¹']}\n```")
                            if doc_idx < len(sorted_docs) - 1:
                                st.markdown("---")
                    else:
                        st.info("æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
            
            # å°†å®Œæ•´å“åº”æ·»åŠ åˆ°ä¼šè¯å†å²
            st.session_state.messages.append({"role": "assistant", "content": response})
            
        except Exception as e:
            error_msg = f"å¤„ç†ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            process_placeholder.error(error_msg)
            message_placeholder.error("ç”Ÿæˆå›ç­”å¤±è´¥ï¼Œè¯·é‡è¯•")
            st.session_state.messages.append({"role": "assistant", "content": f"é”™è¯¯: {error_msg}"})


def flash_llm_chat(question: str):
    """æ™®é€šå¯¹è¯æ¨¡å¼ï¼Œä½¿ç”¨OpenAI APIç›´æ¥æµå¼è¾“å‡º"""
    st.session_state.messages.append({"role": "user", "content": question})

    with st.chat_message("user"):
        st.write(question)

    # åˆ›å»ºç”¨äºæµå¼è¾“å‡ºçš„èŠå¤©æ¶ˆæ¯
    with st.chat_message("assistant"):
        # åˆ›å»ºæ¶ˆæ¯å†å²
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        # æ·»åŠ å†å²æ¶ˆæ¯(æœ€å¤šä¿ç•™5è½®å¯¹è¯)
        history_messages = []
        for msg in st.session_state.messages[-10:]:  # é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡
            if msg["role"] != "system":  # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯
                history_messages.append({"role": msg["role"], "content": msg["content"]})
        
        # æ·»åŠ å†å²æ¶ˆæ¯
        messages.extend(history_messages)
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        client = OpenAI(api_key=OPENAI_API_KEY)
        
        # åˆ›å»ºæµå¼æ–‡æœ¬ç”Ÿæˆå ä½ç¬¦
        placeholder = st.empty()
        full_response = ""
        
        try:
            # ç›´æ¥ä½¿ç”¨OpenAI APIè¿›è¡Œæµå¼ç”Ÿæˆ
            stream = client.chat.completions.create(
                model=selected_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            
            # å¤„ç†æµå¼å“åº”
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    # å®æ—¶æ›´æ–°UI
                    placeholder.markdown(full_response + "â–Œ")  # æ¨¡æ‹Ÿå…‰æ ‡
            
            # æ˜¾ç¤ºå®Œæ•´å“åº”
            placeholder.markdown(full_response)
            
        except Exception as e:
            st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
            full_response = f"å¾ˆæŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"
            placeholder.markdown(full_response)

    # å°†å®Œæ•´å“åº”æ·»åŠ åˆ°ä¼šè¯å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})


# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "system", "content": system_prompt}]

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])


# ç”¨æˆ·è¾“å…¥
if question := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    if deep_search_flag:
        flash_deep_search_workflow(question)
    else:
        flash_llm_chat(question)
