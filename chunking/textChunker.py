import re
import sys

sys.path.append(".")
sys.path.append("..")

from typing import List, Union, Literal, Any
from chunking.baseChunker import BaseChunker
from chunking.baseChunker import Document


class PunctuationChunker(BaseChunker):
    def __init__(
            self, 
            punctuation_set=None
        ):
        super().__init__()
        self.punctuation_set = punctuation_set if punctuation_set else {'.', ',', '!', '?', ';', "ã€‚", "ï¼Œ"}

    def chunk(self, text: str, title: str = "", **kwargs) -> List[Document]:
        """
        æŒ‰å­—ç¬¦åˆ†å—ï¼Œç¡®ä¿åœ¨æ ‡ç‚¹ç¬¦å·å¤„åˆ†å‰²ï¼Œå—å¤§å°åœ¨ min_chunk_size å’Œ max_chunk_size ä¹‹é—´ï¼Œ
        å¹¶æ”¯æŒ overlap_chunk_size çš„é‡å åŠŸèƒ½ã€‚
        """
        min_chunk_size = kwargs.get("min_chunk_size")
        max_chunk_size = kwargs.get("max_chunk_size")
        overlap_chunk_size = kwargs.get("overlap_chunk_size")

        # å‚æ•°éªŒè¯
        if min_chunk_size is None or max_chunk_size is None or overlap_chunk_size is None:
            raise ValueError("min_chunk_size, max_chunk_size, and overlap_chunk_size are required parameters")

        if min_chunk_size <= 0 or max_chunk_size <= 0 or overlap_chunk_size < 0:
            raise ValueError("min_chunk_size å’Œ max_chunk_size å¿…é¡»ä¸ºæ­£æ•°ï¼Œoverlap_chunk_size å¿…é¡»ä¸ºéè´Ÿæ•°")
        if min_chunk_size > max_chunk_size:
            raise ValueError("min_chunk_size å¿…é¡»å°äºæˆ–ç­‰äº max_chunk_size")
        if overlap_chunk_size >= max_chunk_size:
            raise ValueError("overlap_chunk_size å¿…é¡»å°äº max_chunk_size")
        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äº min_chunk_sizeï¼Œç›´æ¥è¿”å›æ•´ä¸ªæ–‡æœ¬
        if len(text) < min_chunk_size:
            return [Document(chunk=text, metadata={"title": title})]

        chunks = []
        start = 0
        text_length = len(text)
        while start < text_length:
            # ç¡®å®šå½“å‰å—çš„ç»“æŸä½ç½®ï¼Œå¿…é¡»åœ¨æ ‡ç‚¹ç¬¦å·å¤„
            end = self._find_next_punctuation(text, start, min_chunk_size, max_chunk_size)
            if end == -1:  # æ²¡æœ‰æ‰¾åˆ°æ ‡ç‚¹ç¬¦å·
                end = text_length  # åˆ†åˆ°æ–‡æœ¬æœ«å°¾
            # æå–å½“å‰å—
            chunk = text[start:end]
            chunks.append(Document(chunk=chunk, metadata={"title": title}))
            # å¦‚æœåˆ°è¾¾æ–‡æœ¬æœ«å°¾ï¼Œé€€å‡ºå¾ªç¯
            if end == text_length:
                break
            # è®¡ç®—ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
            start = end - overlap_chunk_size

        # å¤„ç†æœ€åä¸€ä¸ªå—å°äº min_chunk_size çš„æƒ…å†µ
        if len(chunks) > 1 and len(chunks[-1].chunk) < min_chunk_size:
            last_chunk = chunks.pop()
            chunks[-1] = Document(
                chunk=chunks[-1].chunk + last_chunk.chunk,
                metadata={"title": title}
            )
        return chunks
    
    def _find_next_punctuation(self, text: str, start: int, min_chunk_size: int, max_chunk_size: int):
        """
        åœ¨ [start + min_chunk_size, start + max_chunk_size] èŒƒå›´å†…å¯»æ‰¾ç¬¬ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·çš„ä½ç½®ã€‚
        å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å› -1ã€‚
        """
        min_end = start + min_chunk_size
        max_end = min(start + max_chunk_size, len(text))
        for i in range(min_end, max_end):
            if text[i] in self.punctuation_set:
                return i + 1  # è¿”å›æ ‡ç‚¹ç¬¦å·åçš„ä½ç½®
        # å¦‚æœåœ¨èŒƒå›´å†…æ²¡æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œç»§ç»­å‘åæ‰¾ç¬¬ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·
        for i in range(max_end, len(text)):
            if text[i] in self.punctuation_set:
                return i + 1
        return -1  # æ²¡æœ‰æ‰¾åˆ°æ ‡ç‚¹ç¬¦å·
    

class RecursiveChunker(BaseChunker):
    """ä¸€ä¸ªç®€å•çš„é€’å½’æ–‡æœ¬åˆ‡åˆ†å™¨ï¼Œç»§æ‰¿è‡ª BaseChunkerã€‚"""
    
    def __init__(
        self,
        chunk_size: int = 100,
        separators: List[str] = None,
        keep_separator: Union[bool, Literal["start", "end"]] = True,
        is_separator_regex: bool = False,
        length_function: callable = len,
        overlap_chunk_size: int = 0
    ) -> None:
        """åˆå§‹åŒ– RecursiveChunkerã€‚
        
        Args:
            chunk_size (int): æœ€å¤§å—å¤§å°ï¼Œé»˜è®¤ 100ã€‚
            separators (List[str]): åˆ†éš”ç¬¦åˆ—è¡¨ï¼Œé»˜è®¤ ["\n\n", "\n", " ", ""]ã€‚
            keep_separator (Union[bool, Literal["start", "end"]]): æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦ï¼Œé»˜è®¤ Trueã€‚
            is_separator_regex (bool): åˆ†éš”ç¬¦æ˜¯å¦ä¸ºæ­£åˆ™è¡¨è¾¾å¼ï¼Œé»˜è®¤ Falseã€‚
            length_function (callable): è®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•°ï¼Œé»˜è®¤ lenã€‚
            overlap_chunk_size (int): æ–‡æœ¬å—é‡å çš„å¤§å°ï¼Œé»˜è®¤ä¸º0(ä¸é‡å )ã€‚
        """
        super().__init__()
        self._chunk_size = chunk_size
        self._separators = separators or ["\n\n", "\n", "ã€‚", ".", "ï¼", "ï¼", ";", "ï¼›", "?", "ï¼Ÿ", ",", "ï¼Œ", "ã€", " ", ""]
        self._keep_separator = keep_separator
        self._is_separator_regex = is_separator_regex
        self._length_function = length_function
        self._overlap_chunk_size = overlap_chunk_size

    def _split_text_with_regex(self, text: str, separator: str) -> List[str]:
        """è¾…åŠ©æ–¹æ³•ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ–‡æœ¬ï¼Œå¹¶æ ¹æ® keep_separator å¤„ç†åˆ†éš”ç¬¦ã€‚"""
        if self._keep_separator == "start":
            splits = re.split(f"({separator})", text)
            return [splits[i] + splits[i + 1] if i + 1 < len(splits) else splits[i] for i in range(0, len(splits) - 1, 2)]
        elif self._keep_separator == "end":
            splits = re.split(f"({separator})", text)
            return [splits[i] + splits[i + 1] if i + 1 < len(splits) else splits[i] for i in range(0, len(splits), 2)]
        elif self._keep_separator:
            splits = re.split(f"({separator})", text)
            return [s + sep for s, sep in zip(splits[::2], splits[1::2] + [""])]
        else:
            return re.split(separator, text)

    def _merge_splits(self, splits: List[str], separator: str) -> List[str]:
        """è¾…åŠ©æ–¹æ³•ï¼šåˆå¹¶å°çš„æ–‡æœ¬å—ã€‚"""
        result = []
        current_chunk = ""
        for s in splits:
            if self._length_function(current_chunk + separator + s) < self._chunk_size:
                current_chunk += (separator + s if current_chunk else s)
            else:
                if current_chunk:
                    result.append(current_chunk)
                current_chunk = s
        if current_chunk:
            result.append(current_chunk)
        return result

    def _split_text(self, text: str, separators: List[str]) -> List[str]:
        """æ ¸å¿ƒé€’å½’åˆ‡åˆ†é€»è¾‘ã€‚"""
        final_chunks = []
        # é€‰æ‹©åˆé€‚çš„ separator
        separator = separators[-1]
        new_separators = []
        for i, _s in enumerate(separators):
            _separator = _s if self._is_separator_regex else re.escape(_s)
            if _s == "":
                separator = _s
                break
            if re.search(_separator, text):
                separator = _s
                new_separators = separators[i + 1:]
                break

        # å¤„ç†åˆ†éš”ç¬¦æ˜¯å¦ä¸ºæ­£åˆ™è¡¨è¾¾å¼
        _separator = separator if self._is_separator_regex else re.escape(separator)
        splits = self._split_text_with_regex(text, _separator)

        # åˆå¹¶å’Œé€’å½’åˆ†å‰²
        _good_splits = []
        _separator = "" if self._keep_separator else separator
        for s in splits:
            if self._length_function(s) < self._chunk_size:
                _good_splits.append(s)
            else:
                if _good_splits:
                    merged_text = self._merge_splits(_good_splits, _separator)
                    final_chunks.extend(merged_text)
                    _good_splits = []
                if not new_separators:
                    final_chunks.append(s)
                else:
                    other_info = self._split_text(s, new_separators)
                    final_chunks.extend(other_info)
        if _good_splits:
            merged_text = self._merge_splits(_good_splits, _separator)
            final_chunks.extend(merged_text)

        return final_chunks

    def chunk(self, text: str, title: str = "", **kwargs) -> List[Document]:
        """å®ç° BaseChunker çš„æŠ½è±¡æ–¹æ³•ï¼Œç”¨äºå°†æ–‡æœ¬åˆ‡åˆ†æˆå°å—ã€‚
        
        Args:
            text (str): è¦åˆ‡åˆ†çš„æ–‡æœ¬
            title (str): æ–‡æ¡£æ ‡é¢˜ï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²
            **kwargs: å¯é€‰å‚æ•°ï¼Œå¯åŒ…å« overlap_chunk_size è¦†ç›–åˆå§‹è®¾ç½®çš„é‡å å¤§å°ã€‚
        
        Returns:
            List[Document]: åŒ…å«åˆ†å‰²åçš„æ–‡æœ¬å—å’Œå…ƒæ•°æ®çš„æ–‡æ¡£å¯¹è±¡åˆ—è¡¨ã€‚
        """
        # å…è®¸é€šè¿‡kwargsè¦†ç›–åˆå§‹è®¾ç½®çš„overlap_chunk_size
        overlap_chunk_size = kwargs.get("overlap_chunk_size", self._overlap_chunk_size)
        
        chunks = self._split_text(text, self._separators)
        # è¿‡æ»¤æ‰ç©ºçš„chunk
        chunks = [chunk for chunk in chunks if chunk.strip()]
        
        # å¦‚æœæ²¡æœ‰è®¾ç½®é‡å æˆ–åªæœ‰ä¸€ä¸ªchunkï¼Œç›´æ¥è¿”å›
        if overlap_chunk_size <= 0 or len(chunks) <= 1:
            documents = [Document(chunk=chunk, metadata={"title": title}) for chunk in chunks]
            return documents
        
        # å¤„ç†é‡å 
        documents = []
        for i, chunk in enumerate(chunks):
            # è·å–æ‰©å±•æ–‡æœ¬
            expanded_chunk = chunk
            
            # å‘ä¸Šæ–‡æ‰©å±•
            if i > 0:
                prev_chunk = chunks[i-1]
                # è®¡ç®—æœ€å¤§å¯æ‰©å±•é•¿åº¦
                max_overlap = min(overlap_chunk_size, len(prev_chunk))
                
                # ä»åå‘å‰åœ¨å‰ä¸€ä¸ªchunkä¸­å¯»æ‰¾åˆ†éš”ç¬¦
                overlap_start = len(prev_chunk) - max_overlap
                prefix = prev_chunk[overlap_start:]
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ†éš”ç¬¦ï¼Œå¦‚æœåŒ…å«ï¼Œä»åˆ†éš”ç¬¦å¤„æˆªæ–­
                for separator in self._separators:
                    if separator and separator in prefix:
                        # æ‰¾åˆ°æœ€åä¸€ä¸ªåˆ†éš”ç¬¦çš„ä½ç½®
                        sep_pos = prefix.rfind(separator)
                        if sep_pos != -1:
                            # ä»åˆ†éš”ç¬¦åå¼€å§‹å–æ–‡æœ¬
                            prefix = prefix[sep_pos + len(separator):]
                            break
                
                expanded_chunk = prefix + expanded_chunk
                
            # å‘ä¸‹æ–‡æ‰©å±•
            if i < len(chunks) - 1:
                next_chunk = chunks[i+1]
                # è®¡ç®—æœ€å¤§å¯æ‰©å±•é•¿åº¦
                max_overlap = min(overlap_chunk_size, len(next_chunk))
                
                # å–ä¸‹ä¸€ä¸ªchunkçš„å‰max_overlapä¸ªå­—ç¬¦
                suffix = next_chunk[:max_overlap]
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ†éš”ç¬¦ï¼Œå¦‚æœåŒ…å«ï¼Œä»åˆ†éš”ç¬¦å¤„æˆªæ–­
                for separator in self._separators:
                    if separator and separator in suffix:
                        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦çš„ä½ç½®
                        sep_pos = suffix.find(separator)
                        if sep_pos != -1:
                            # åªå–åˆ°åˆ†éš”ç¬¦å‰çš„æ–‡æœ¬
                            suffix = suffix[:sep_pos]
                            break
                
                expanded_chunk = expanded_chunk + suffix
            
            documents.append(Document(chunk=expanded_chunk, metadata={"title": title}))
        
        return documents
    

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # æµ‹è¯•æ–‡æœ¬
    sample_text = """
### <center>è°ƒç ”æŠ¥å‘Šï¼šDeepSeek-R1 ä¸ Agents æ¡†æ¶ç»“åˆæ•ˆç‡åˆ†æ<center/>

#### ä¸€å¥è¯æ€»ç»“

DeepSeek-R1 åœ¨å¤šæ¬¡è°ƒç”¨ ReAct æµç¨‹ä¸­ï¼Œé¦–æ¬¡ Long-CoT ç¨é•¿ä½†åç»­é«˜æ•ˆï¼Œæ•´ä½“æ€§èƒ½ç¨³å®šï¼Œé€‚åº”å½“å‰å¼€æºæ¡†æ¶ç°çŠ¶å¹¶å±•ç°ä¼˜è¶Šæ¨ç†æ€§èƒ½ã€‚

#### èƒŒæ™¯ä¸ç›®æ ‡

æœ¬æ–‡è°ƒç ” DeepSeek-R1 ä¸ Agents æ¡†æ¶ç»“åˆçš„æ•ˆæœï¼ŒåŸºäº Dify åº•å±‚ä»£ç åˆ†æå…¶ Agents Workflow æµç¨‹ï¼ŒéªŒè¯ DeepSeek-R1 åœ¨å¤šæ¬¡ Function Calling åœºæ™¯ä¸‹çš„æ¨ç†æ•ˆç‡ã€‚ç›®æ ‡æ˜¯è¯„ä¼° Long-CoTï¼ˆé•¿é“¾æ¨ç†ï¼‰å’Œåç»­ Function Calling å¯¹æ•´ä½“æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¢è®¨DeepSeek-R1å½“å‰å¯¹ ReAct agentæµç¨‹çš„æ”¯æŒç°çŠ¶ã€‚

#### Dify Agents Workflow æµç¨‹

é€šè¿‡åˆ†æ Dify åº•å±‚ä»£ç ï¼Œå…¶å·¥ä½œæµä¸ºä¸€ä¸ªå¾ªç¯è¿‡ç¨‹ï¼š

1. æ¨¡å‹ç”Ÿæˆå“åº”ï¼ˆå¯èƒ½åŒ…å«å·¥å…·è°ƒç”¨ï¼‰ã€‚
2. è‹¥å­˜åœ¨å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·å¹¶è·å–ç»“æœã€‚
3. å°†å·¥å…·è°ƒç”¨ç»“æœåŠ å…¥å¯¹è¯å†å²ã€‚
4. é‡æ–°ç»„ç»‡æç¤ºè¯ï¼ˆèå…¥å·¥å…·ç»“æœï¼‰ã€‚
5. å†æ¬¡è°ƒç”¨æ¨¡å‹æ¨ç†ã€‚
6. é‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°æ¨¡å‹ä¸å†è°ƒç”¨å·¥å…·æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚

**è¡¥å……è¯´æ˜**ï¼šä¸ç†æƒ³çš„ ReAct æµç¨‹ï¼ˆThoughtã€Actionã€Observation æ‹¼æ¥ä¸ºå•æ¬¡æ¨¡å‹è°ƒç”¨ï¼‰ä¸åŒï¼ŒDify åŠå¤§éƒ¨åˆ†å¼€æºæ¡†æ¶é‡‡ç”¨å¤šæ¬¡æ¨¡å‹è°ƒç”¨çš„å•è½® ReAct æ–¹å¼ï¼Œå³æ¯æ¬¡ä»…å¤„ç†ä¸€ä¸ªç¯èŠ‚ï¼Œä¾èµ–å¾ªç¯é€æ­¥å®Œæˆä»»åŠ¡ã€‚æ‰€ä»¥è®¾æƒ³çš„deepseekèƒ½å¤Ÿé€šè¿‡æ‹¼æ¥çš„æ–¹å¼å®ç°æ•ˆç‡æå‡ï¼ˆèŠ‚çœthinkingè¿‡ç¨‹ï¼‰çš„æƒ³æ³•æš‚ä¸ç°å®ã€‚

#### å®éªŒè®¾è®¡

- **æ¨¡å‹**ï¼šDeepSeek-R1 ä½œä¸ºæ ¸å¿ƒå¤§æ¨¡å‹ã€‚
- **åœºæ™¯**ï¼šæ¨¡æ‹Ÿå¤šæ¬¡ Function Calling çš„ä»»åŠ¡ï¼ˆå¦‚ä¿¡æ¯æ£€ç´¢ã€æ•°æ®å¤„ç†ï¼‰ã€‚
- **å…³æ³¨ç‚¹**ï¼šæ¨ç†æ—¶é—´ï¼Œå°¤å…¶æ˜¯ Long-CoT å’Œåç»­ Function Calling çš„æ—¶é•¿ã€‚
- **å‡è®¾**ï¼šæ‹…å¿ƒå¤šæ¬¡æ¨¡å‹è°ƒç”¨ï¼ˆç‰¹åˆ«æ˜¯ Long-CoTï¼‰ä¼šå› æ¨ç†è¿‡é•¿é™ä½æ•ˆç‡ã€‚

#### å®éªŒç»“æœ

1. **å®éªŒdemo(ä½¿ç”¨DeepSeek-R1)**ï¼š

   ```
   ğŸ‰ æœ€ç»ˆç»“æœ:
   ========================================
   æ ¹æ®2025å¹´2æœˆ27æ—¥Windæ•°æ®ï¼š
   1ï¸âƒ£ è´µå·èŒ…å°æ”¶ç›˜ä»·ï¼š1823å…ƒ
   2ï¸âƒ£ äº”ç²®æ¶²å¼€ç›˜ä»·ï¼š140å…ƒ
   3ï¸âƒ£ ä»Šæ—¥çƒ­ç‚¹æ–°é—»å‰åï¼š
   â‘  å…¨çƒé¦–è¾†å•†ä¸šé£è¡Œæ±½è½¦æ­£å¼ä¸Šè·¯æµ‹è¯•æˆåŠŸ  
   â‘¡ æŸç§‘æŠ€å…¬å¸å®£å¸ƒæ¨å‡ºé¦–æ¬¾é‡å­æ‰‹æœº  
   â‘¢ ä¸–ç•Œæ¯é¢„é€‰èµ›çˆ†å†·ï¼Œå¼ºé˜Ÿæ„å¤–å¤±åˆ©  
   â‘£ ä¸€çº¿åŸå¸‚æˆ¿ä»·ç¯æ¯”ä¸‹é™3%ï¼Œå¼•å‘çƒ­è®®  
   â‘¤ æŸå›½æ¨å‡ºå…¨çƒé¦–ä¸ªå…¨æ°‘åŸºç¡€æ”¶å…¥è¯•ç‚¹è®¡åˆ’  
   â‘¥ å…¨çƒæ°”æ¸©åˆ›æ–°é«˜ï¼Œæ°”å€™å˜åŒ–è®®é¢˜å†å¼•å…³æ³¨  
   â‘¦ æŸçŸ¥åAIæœºå™¨äººå…¬å¸è¢«æ›éšç§æ³„éœ²é—®é¢˜  
   â‘§ æŸå›½æˆåŠŸå‘å°„è½½äººæ¢æµ‹å™¨å‰å¾€ç«æ˜Ÿ  
   â‘¨ æ–°èƒ½æºè½¦ç”µæ± æŠ€æœ¯çªç ´ï¼Œç»­èˆªé‡Œç¨‹ç¿»å€  
   â‘© æŸæµè¡Œæ­Œæ‰‹æ–°ä¸“è¾‘24å°æ—¶é”€é‡ç ´çºªå½•
   ========================================
   ```

2. **é¦–æ¬¡è°ƒç”¨**ï¼š

   - DeepSeek-R1 åœ¨é¦–æ¬¡è°ƒç”¨æ—¶ç”Ÿæˆ Long-CoTï¼Œæ¨ç†æ—¶é—´è¾ƒé•¿ï¼ˆå› ä»»åŠ¡å¤æ‚åº¦è€Œå¼‚ï¼Œä½†æ˜¾è‘—é«˜äºåç»­è°ƒç”¨ï¼‰ã€‚

   - åŸå› ï¼šé¦–æ¬¡è°ƒç”¨éœ€å®Œæ•´åˆ†è§£é—®é¢˜å¹¶ç”Ÿæˆè¯¦ç»†æ¨ç†é“¾ã€‚

3. **åç»­ Function Calling**ï¼š

   - å·¥å…·è°ƒç”¨åï¼Œæ¨¡å‹åŸºäºå·²æœ‰ä¸Šä¸‹æ–‡å’Œç»“æœæ¨ç†ã€‚

   - Thinking å’Œ Reasoning æ—¶é—´æ˜¾è‘—ç¼©çŸ­ï¼Œä»…ä¸ºé¦–æ¬¡è°ƒç”¨çš„ 10%-20%ã€‚

   - åŸå› ï¼šåç»­è°ƒç”¨ä¾èµ–å·²æœ‰æ¨ç†åŸºç¡€ï¼Œå‡å°‘é‡å¤è®¡ç®—ã€‚

4. **æ•ˆç‡å½±å“**ï¼š

   - Long-CoT ä»…åœ¨é¦–æ¬¡è°ƒç”¨æ—¶æ˜¾è‘—ï¼Œåç»­ Function Calling çš„çŸ­æ¨ç†ä¸æ‹–ç´¯æ•´ä½“æ•ˆç‡ã€‚
   - å¤šæ¬¡æ¨¡å‹è°ƒç”¨è™½éå•æ¬¡ ReActï¼Œä½†è¿­ä»£æ•ˆç‡ä»é«˜ï¼Œæœªè§æ€§èƒ½æ˜æ˜¾ä¸‹é™ã€‚

#### ç»“è®ºä¸äº®ç‚¹

- **é«˜æ•ˆæ€§éªŒè¯**ï¼šLong-CoT é›†ä¸­äºé¦–æ¬¡è°ƒç”¨ï¼Œåç»­ Function Calling é«˜æ•ˆã€‚
- **ç°çŠ¶åæ€**ï¼šç›¸æ¯”äºåŸæœ‰çš„éreasoningç³»åˆ—æ¨¡å‹ï¼ŒDeepSeek-R1 çš„çŸ­æ¨ç†ç‰¹æ€§å¼¥è¡¥äº†æ•ˆæœçŸ­æ¿ã€‚
- **é€‚ç”¨æ€§**ï¼šé€‚åˆå¤æ‚å¤šè½®å·¥å…·è°ƒç”¨ä»»åŠ¡ï¼Œæ— éœ€æ‹…å¿ƒæ¨ç†æ—¶é•¿ç´¯ç§¯ã€‚
"""
    
    # # åˆ›å»º RecursiveChunker å®ä¾‹
    # chunker = RecursiveChunker(chunk_size=200)
    
    # # è°ƒç”¨ chunk æ–¹æ³•
    # documents = chunker.chunk(text=sample_text, title="æµ‹è¯•æ–‡æ¡£.txt")
    
    # # è¾“å‡ºç»“æœ
    # print(f"æ–‡æ¡£æ•°é‡: {len(documents)}")
    # for i, doc in enumerate(documents):
    #     print(f"chunk {i}: {doc.chunk}")
    #     print(f"metadata: {doc.metadata}")
    #     print(f"formatted chunk: \n{doc.format_chunk()}")
    #     print("-" * 50)

    # # è®¡ç®—å¹³å‡é•¿åº¦
    # average_length = sum(len(doc.chunk) for doc in documents) / len(documents)
    # print(f"å¹³å‡é•¿åº¦: {average_length}")

    # print("=" * 50)
    # print("æµ‹è¯• PunctuationChunker:")
    # # åˆ›å»º PunctuationChunker å®ä¾‹
    # punctuation_chunker = PunctuationChunker()
    # documents = punctuation_chunker.chunk(
    #     text=sample_text, 
    #     title="æµ‹è¯•æ–‡æ¡£", 
    #     min_chunk_size=100, 
    #     max_chunk_size=200, 
    #     overlap_chunk_size=50
    # )
    
    # # è¾“å‡ºç»“æœ
    # print(f"æ–‡æ¡£æ•°é‡: {len(documents)}")
    # for i, doc in enumerate(documents):
    #     print(f"chunk {i}: {doc.chunk}")
    #     print(f"metadata: {doc.metadata}")
    #     print(f"formatted chunk: \n{doc.format_chunk()}")
    #     print("-" * 50)
    
    print("=" * 50)
    print("æµ‹è¯• RecursiveChunker çš„ overlap_chunk_size åŠŸèƒ½:")
    # åˆ›å»ºå¸¦æœ‰é‡å çš„ RecursiveChunker å®ä¾‹
    overlap_chunker = RecursiveChunker(chunk_size=200, overlap_chunk_size=50)
    
    # è°ƒç”¨ chunk æ–¹æ³•
    documents = overlap_chunker.chunk(text=sample_text, title="æµ‹è¯•æ–‡æ¡£-é‡å ç‰ˆ.txt")
    
    # è¾“å‡ºç»“æœ
    print(f"æ–‡æ¡£æ•°é‡: {len(documents)}")
    for i, doc in enumerate(documents):
        print(f"chunk {i}: {doc.chunk}")
        print(f"metadata: {doc.metadata}")
        print(f"formatted chunk: \n{doc.format_chunk()}")
        print("-" * 50)
    
    # è®¡ç®—å¸¦é‡å çš„å¹³å‡é•¿åº¦
    average_length = sum(len(doc.chunk) for doc in documents) / len(documents)
    print(f"å¸¦é‡å çš„å¹³å‡é•¿åº¦: {average_length}")
    
    # # æµ‹è¯•é€šè¿‡kwargsè®¾ç½®overlap_chunk_size
    # print("=" * 50)
    # print("æµ‹è¯•é€šè¿‡kwargsè®¾ç½®overlap_chunk_size:")
    # chunker = RecursiveChunker(chunk_size=200)  # åˆå§‹ä¸è®¾ç½®é‡å 
    # documents = chunker.chunk(
    #     text=sample_text, 
    #     title="æµ‹è¯•æ–‡æ¡£-kwargsé‡å ç‰ˆ.txt",
    #     overlap_chunk_size=30  # é€šè¿‡kwargsè®¾ç½®é‡å 
    # )
    
    # print(f"æ–‡æ¡£æ•°é‡: {len(documents)}")
    # for i, doc in enumerate(documents):
    #     print(f"chunk {i}: {doc.chunk}")
    #     print(f"metadata: {doc.metadata}")
    #     print("-" * 50)
    
    # # è®¡ç®—é€šè¿‡kwargsè®¾ç½®é‡å çš„å¹³å‡é•¿åº¦
    # average_length = sum(len(doc.chunk) for doc in documents) / len(documents)
    # print(f"é€šè¿‡kwargsè®¾ç½®é‡å çš„å¹³å‡é•¿åº¦: {average_length}")