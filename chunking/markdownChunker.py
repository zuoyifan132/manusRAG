import sys

sys.path.append(".")
sys.path.append("..")

from typing import List, Union, Literal, Any, Dict
from chunking.baseChunker import BaseChunker
from chunking.baseChunker import Document
from chunking.textChunker import RecursiveChunker


class MarkdownChunker(BaseChunker):
    """Markdownæ–‡æ¡£æ ‡é¢˜åˆ†å‰²å™¨
    
    ç”¨äºæ ¹æ®æŒ‡å®šçš„æ ‡é¢˜å±‚çº§å¯¹Markdownæ–‡æ¡£è¿›è¡Œåˆ†å‰²ï¼Œæ¯ä¸ªåˆ†å‰²åçš„å—éƒ½åŒ…å«ç›¸åº”çš„æ ‡é¢˜å…ƒæ•°æ®ã€‚
    """

    def __init__(
        self,
        markdown_headers_to_split_on: List[tuple[str, str]],
        return_each_line: bool = False,
        strip_headers: bool = True,
        markdown_chunk_limit: int = 200,
    ):
        """åˆå§‹åŒ–Markdownæ ‡é¢˜åˆ†å‰²å™¨

        Args:
            markdown_headers_to_split_on: è¦åˆ†å‰²çš„æ ‡é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(æ ‡é¢˜æ ‡è®°, æ ‡é¢˜åç§°)çš„å…ƒç»„
                ä¾‹å¦‚: [("#", "h1"), ("##", "h2")]
            return_each_line: æ˜¯å¦è¿”å›æ¯è¡Œå†…å®¹ï¼Œå¦‚æœä¸ºFalseåˆ™è¿”å›èšåˆåçš„å†…å®¹å—
            strip_headers: æ˜¯å¦ä»å†…å®¹ä¸­ç§»é™¤æ ‡é¢˜è¡Œ
            markdown_chunk_limit: å½“å—é•¿åº¦è¶…è¿‡æ­¤å€¼æ—¶ï¼Œä½¿ç”¨ RecursiveChunker è¿›è¡Œè¿›ä¸€æ­¥åˆ‡åˆ†
        """
        super().__init__()
        self.return_each_line = return_each_line
        # å¯¹æ ‡é¢˜æ ‡è®°æŒ‰é•¿åº¦é™åºæ’åºï¼Œç¡®ä¿æ›´é•¿çš„æ ‡é¢˜æ ‡è®°ä¼˜å…ˆåŒ¹é…
        self.markdown_headers_to_split_on = sorted(
            markdown_headers_to_split_on, key=lambda split: len(split[0]), reverse=True
        )
        self.strip_headers = strip_headers
        self.markdown_chunk_limit = markdown_chunk_limit
        self.recursive_chunker = RecursiveChunker(chunk_size=markdown_chunk_limit)

    def aggregate_lines_to_chunks(
        self, lines: List[Dict[str, Union[str, Dict[str, str]]]]
    ) -> List[Dict[str, Union[str, Dict[str, str]]]]:
        """å°†å…·æœ‰ç›¸åŒå…ƒæ•°æ®çš„è¿ç»­è¡Œèšåˆä¸ºå†…å®¹å—

        Args:
            lines: åŒ…å«å†…å®¹å’Œå…ƒæ•°æ®çš„è¡Œåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º{"content": å†…å®¹, "metadata": å…ƒæ•°æ®}

        Returns:
            èšåˆåçš„å†…å®¹å—åˆ—è¡¨
        """
        aggregated_chunks: List[Dict[str, Union[str, Dict[str, str]]]] = []

        for line in lines:
            # å¦‚æœå½“å‰è¡Œä¸ä¸Šä¸€ä¸ªå—çš„å…ƒæ•°æ®ç›¸åŒï¼Œåˆ™åˆå¹¶å†…å®¹
            if (
                aggregated_chunks
                and aggregated_chunks[-1]["metadata"] == line["metadata"]
            ):
                aggregated_chunks[-1]["content"] += "  \n" + line["content"]
            # å¤„ç†æ ‡é¢˜å±‚çº§å˜åŒ–çš„æƒ…å†µ
            elif (
                aggregated_chunks
                and aggregated_chunks[-1]["metadata"] != line["metadata"]
                and len(aggregated_chunks[-1]["metadata"]) < len(line["metadata"])
                and aggregated_chunks[-1]["content"].split("\n")[-1][0] == "#"
                and not self.strip_headers
            ):
                aggregated_chunks[-1]["content"] += "  \n" + line["content"]
                aggregated_chunks[-1]["metadata"] = line["metadata"]
            # åˆ›å»ºæ–°çš„å†…å®¹å—
            else:
                aggregated_chunks.append(line)

        return aggregated_chunks
    
    def split_text(self, text: str) -> List[Dict[str, Union[str, Dict[str, str]]]]:
        """åˆ†å‰²Markdownæ–‡æœ¬

        Args:
            text: è¦åˆ†å‰²çš„Markdownæ–‡æœ¬

        Returns:
            åˆ†å‰²åçš„å†…å®¹å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å«å†…å®¹å’Œå…ƒæ•°æ®
        """
        # æŒ‰è¡Œåˆ†å‰²æ–‡æœ¬
        lines = text.split("\n")
        # å­˜å‚¨å¸¦å…ƒæ•°æ®çš„è¡Œ
        lines_with_metadata: List[Dict[str, Union[str, Dict[str, str]]]] = []
        # å½“å‰æ­£åœ¨æ”¶é›†çš„å†…å®¹
        current_content: List[str] = []
        # å½“å‰å—çš„å…ƒæ•°æ®
        current_metadata: Dict[str, str] = {}
        # æ ‡é¢˜æ ˆï¼Œç”¨äºç»´æŠ¤æ ‡é¢˜å±‚çº§
        header_stack: List[Dict[str, Union[int, str]]] = []
        # åˆå§‹å…ƒæ•°æ®
        initial_metadata: Dict[str, str] = {}

        # ä»£ç å—å¤„ç†ç›¸å…³å˜é‡
        in_code_block = False
        opening_fence = ""

        for line in lines:
            # æ¸…ç†è¡Œå†…å®¹
            stripped_line = line.strip()
            stripped_line = "".join(filter(str.isprintable, stripped_line))
            
            # å¤„ç†ä»£ç å—
            if not in_code_block:
                if stripped_line.startswith("```") and stripped_line.count("```") == 1:
                    in_code_block = True
                    opening_fence = "```"
                elif stripped_line.startswith("~~~"):
                    in_code_block = True
                    opening_fence = "~~~"
            else:
                if stripped_line.startswith(opening_fence):
                    in_code_block = False
                    opening_fence = ""

            # å¦‚æœæ˜¯ä»£ç å—ä¸­çš„å†…å®¹ï¼Œç›´æ¥æ·»åŠ åˆ°å½“å‰å†…å®¹
            if in_code_block:
                current_content.append(stripped_line)
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
            for sep, name in self.markdown_headers_to_split_on:
                if stripped_line.startswith(sep) and (
                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == " "
                ):
                    if name is not None:
                        # è®¡ç®—å½“å‰æ ‡é¢˜å±‚çº§
                        current_header_level = sep.count("#")
                        # æ¸…ç†æ¯”å½“å‰å±‚çº§é«˜çš„æ ‡é¢˜
                        while (
                            header_stack
                            and header_stack[-1]["level"] >= current_header_level
                        ):
                            popped_header = header_stack.pop()
                            if popped_header["name"] in initial_metadata:
                                initial_metadata.pop(popped_header["name"])

                        # æ·»åŠ æ–°æ ‡é¢˜åˆ°æ ˆä¸­
                        header = {
                            "level": current_header_level,
                            "name": name,
                            "data": stripped_line[len(sep):].strip(),
                        }
                        header_stack.append(header)
                        initial_metadata[name] = header["data"]

                    # ä¿å­˜å½“å‰æ”¶é›†çš„å†…å®¹
                    if current_content:
                        lines_with_metadata.append(
                            {
                                "content": "\n".join(current_content),
                                "metadata": current_metadata.copy(),
                            }
                        )
                        current_content.clear()

                    # å¦‚æœä¸ç§»é™¤æ ‡é¢˜ï¼Œå°†æ ‡é¢˜æ·»åŠ åˆ°å†…å®¹ä¸­
                    if not self.strip_headers:
                        current_content.append(stripped_line)

                    break
            else:
                # å¤„ç†æ™®é€šæ–‡æœ¬è¡Œ
                if stripped_line:
                    current_content.append(stripped_line)
                elif current_content:
                    # é‡åˆ°ç©ºè¡Œæ—¶ä¿å­˜å½“å‰å†…å®¹
                    lines_with_metadata.append(
                        {
                            "content": "\n".join(current_content),
                            "metadata": current_metadata.copy(),
                        }
                    )
                    current_content.clear()

            # æ›´æ–°å½“å‰å…ƒæ•°æ®
            current_metadata = initial_metadata.copy()

        # å¤„ç†æœ€åçš„å†…å®¹å—
        if current_content:
            lines_with_metadata.append(
                {
                    "content": "\n".join(current_content),
                    "metadata": current_metadata,
                }
            )

        # æ ¹æ®è®¾ç½®è¿”å›æ¯è¡Œå†…å®¹æˆ–èšåˆåçš„å†…å®¹å—
        if not self.return_each_line:
            return self.aggregate_lines_to_chunks(lines_with_metadata)
        else:
            return lines_with_metadata
        
    def chunk(self, text: str, title: str = "", **kwargs) -> List[Document]:
        """å®ç° BaseChunker çš„æŠ½è±¡æ–¹æ³•ï¼Œç”¨äºå°†Markdownæ–‡æœ¬åˆ‡åˆ†æˆå°å—ã€‚
        
        Args:
            text (str): è¦åˆ‡åˆ†çš„Markdownæ–‡æœ¬
            title (str): æ–‡æ¡£æ ‡é¢˜ï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²
            **kwargs: å¯é€‰å‚æ•°ï¼Œå½“å‰æœªä½¿ç”¨ã€‚
        
        Returns:
            List[Document]: åŒ…å«åˆ†å‰²åçš„å†…å®¹å—å’Œå…ƒæ•°æ®çš„æ–‡æ¡£å¯¹è±¡åˆ—è¡¨ã€‚
        """
        chunks = self.split_text(text)
        # åˆ›å»ºæ–‡æ¡£åˆ—è¡¨
        documents = []
        for chunk in chunks:
            # åˆ›å»ºå…ƒæ•°æ®å­—å…¸ï¼Œç¡®ä¿ title åœ¨é¦–ä½
            metadata = {"title": title}
            # æ·»åŠ å…¶ä»–å…ƒæ•°æ®
            metadata.update(chunk["metadata"])
            
            # å¦‚æœå†…å®¹é•¿åº¦è¶…è¿‡é™åˆ¶ï¼Œä½¿ç”¨ RecursiveChunker è¿›è¡Œè¿›ä¸€æ­¥åˆ‡åˆ†
            if len(chunk["content"]) > self.markdown_chunk_limit:
                recursive_docs = self.recursive_chunker.chunk(
                    text=chunk["content"],
                    title=title,
                    **kwargs
                )
                # ä¸ºæ¯ä¸ªé€’å½’åˆ‡åˆ†çš„æ–‡æ¡£æ·»åŠ åŸå§‹å…ƒæ•°æ®
                for doc in recursive_docs:
                    doc.metadata.update(metadata)
                documents.extend(recursive_docs)
            else:
                # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
                doc = Document(chunk=chunk["content"], metadata=metadata)
                documents.append(doc)
        return documents
    

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # æµ‹è¯•æ–‡æœ¬
    sample_text = """
### <center>è°ƒç ”æŠ¥å‘Šï¼šDeepSeek-R1 ä¸ Agents æ¡†æ¶ç»“åˆæ•ˆç‡åˆ†æ<center/>

#### ä¸€å¥è¯æ€»ç»“

DeepSeek-R1 åœ¨å¤šæ¬¡è°ƒç”¨ ReAct æµç¨‹ä¸­ï¼Œé¦–æ¬¡ Long-CoT ç¨é•¿ä½†åç»­é«˜æ•ˆï¼Œæ•´ä½“æ€§èƒ½ç¨³å®šï¼Œé€‚åº”å½“å‰å¼€æºæ¡†æ¶ç°çŠ¶å¹¶å±•ç°ä¼˜è¶Šæ¨ç†æ€§èƒ½ã€‚

#### èƒŒæ™¯ä¸ç›®æ ‡

æœ¬æ–‡è°ƒç ” DeepSeek-R1 ä¸ Agents æ¡†æ¶ç»“åˆçš„æ•ˆæœï¼ŒåŸºäº Dify åº•å±‚ä»£ç åˆ†æå…¶ Agents Workflow æµç¨‹ï¼ŒéªŒè¯ DeepSeek-R1 åœ¨å¤šæ¬¡ Function Calling åœºæ™¯ä¸‹çš„æ¨ç†æ•ˆç‡ã€‚ç›®æ ‡æ˜¯è¯„ä¼° Long-CoTï¼ˆé•¿é“¾æ¨ç†ï¼‰å’Œåç»­ Function Calling å¯¹æ•´ä½“æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¢è®¨DeepSeek-R1å½“å‰å¯¹ ReAct agentæµç¨‹çš„æ”¯æŒç°çŠ¶ã€‚

#### Dify Agents Workflow æµç¨‹

é€šè¿‡åˆ†æ Dify åº•å±‚ä»£ç ï¼Œå…¶å·¥ä½œæµä¸ºä¸€ä¸ªå¾ªç¯è¿‡ç¨‹ï¼š

1. æ¨¡å‹ç”Ÿæˆå“åº”ï¼ˆå¯èƒ½åŒ…å«å·¥å…·è°ƒç”¨ï¼‰ã€‚
2. è‹¥å­˜åœ¨å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·å¹¶è·å–ç»“æœã€‚
3. å°†å·¥å…·è°ƒç”¨ç»“æœåŠ å…¥å¯¹è¯å†å²ã€‚
4. é‡æ–°ç»„ç»‡æç¤ºè¯ï¼ˆèå…¥å·¥å…·ç»“æœï¼‰ã€‚
5. å†æ¬¡è°ƒç”¨æ¨¡å‹æ¨ç†ã€‚
6. é‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°æ¨¡å‹ä¸å†è°ƒç”¨å·¥å…·æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚

**è¡¥å……è¯´æ˜**ï¼šä¸ç†æƒ³çš„ ReAct æµç¨‹ï¼ˆThoughtã€Actionã€Observation æ‹¼æ¥ä¸ºå•æ¬¡æ¨¡å‹è°ƒç”¨ï¼‰ä¸åŒï¼ŒDify åŠå¤§éƒ¨åˆ†å¼€æºæ¡†æ¶é‡‡ç”¨å¤šæ¬¡æ¨¡å‹è°ƒç”¨çš„å•è½® ReAct æ–¹å¼ï¼Œå³æ¯æ¬¡ä»…å¤„ç†ä¸€ä¸ªç¯èŠ‚ï¼Œä¾èµ–å¾ªç¯é€æ­¥å®Œæˆä»»åŠ¡ã€‚æ‰€ä»¥è®¾æƒ³çš„deepseekèƒ½å¤Ÿé€šè¿‡æ‹¼æ¥çš„æ–¹å¼å®ç°æ•ˆç‡æå‡ï¼ˆèŠ‚çœthinkingè¿‡ç¨‹ï¼‰çš„æƒ³æ³•æš‚ä¸ç°å®ã€‚

#### å®éªŒè®¾è®¡

- **æ¨¡å‹**ï¼šDeepSeek-R1 ä½œä¸ºæ ¸å¿ƒå¤§æ¨¡å‹ã€‚
- **åœºæ™¯**ï¼šæ¨¡æ‹Ÿå¤šæ¬¡ Function Calling çš„ä»»åŠ¡ï¼ˆå¦‚ä¿¡æ¯æ£€ç´¢ã€æ•°æ®å¤„ç†ï¼‰ã€‚
- **å…³æ³¨ç‚¹**ï¼šæ¨ç†æ—¶é—´ï¼Œå°¤å…¶æ˜¯ Long-CoT å’Œåç»­ Function Calling çš„æ—¶é•¿ã€‚
- **å‡è®¾**ï¼šæ‹…å¿ƒå¤šæ¬¡æ¨¡å‹è°ƒç”¨ï¼ˆç‰¹åˆ«æ˜¯ Long-CoTï¼‰ä¼šå› æ¨ç†è¿‡é•¿é™ä½æ•ˆç‡ã€‚

#### å®éªŒç»“æœ

1. **å®éªŒdemo(ä½¿ç”¨DeepSeek-R1)**ï¼š

   ```
   ğŸ‰ æœ€ç»ˆç»“æœ:
   ========================================
   æ ¹æ®2025å¹´2æœˆ27æ—¥Windæ•°æ®ï¼š
   1ï¸âƒ£ è´µå·èŒ…å°æ”¶ç›˜ä»·ï¼š1823å…ƒ
   2ï¸âƒ£ äº”ç²®æ¶²å¼€ç›˜ä»·ï¼š140å…ƒ
   3ï¸âƒ£ ä»Šæ—¥çƒ­ç‚¹æ–°é—»å‰åï¼š
   â‘  å…¨çƒé¦–è¾†å•†ä¸šé£è¡Œæ±½è½¦æ­£å¼ä¸Šè·¯æµ‹è¯•æˆåŠŸ  
   â‘¡ æŸç§‘æŠ€å…¬å¸å®£å¸ƒæ¨å‡ºé¦–æ¬¾é‡å­æ‰‹æœº  
   â‘¢ ä¸–ç•Œæ¯é¢„é€‰èµ›çˆ†å†·ï¼Œå¼ºé˜Ÿæ„å¤–å¤±åˆ©  
   â‘£ ä¸€çº¿åŸå¸‚æˆ¿ä»·ç¯æ¯”ä¸‹é™3%ï¼Œå¼•å‘çƒ­è®®  
   â‘¤ æŸå›½æ¨å‡ºå…¨çƒé¦–ä¸ªå…¨æ°‘åŸºç¡€æ”¶å…¥è¯•ç‚¹è®¡åˆ’  
   â‘¥ å…¨çƒæ°”æ¸©åˆ›æ–°é«˜ï¼Œæ°”å€™å˜åŒ–è®®é¢˜å†å¼•å…³æ³¨  
   â‘¦ æŸçŸ¥åAIæœºå™¨äººå…¬å¸è¢«æ›éšç§æ³„éœ²é—®é¢˜  
   â‘§ æŸå›½æˆåŠŸå‘å°„è½½äººæ¢æµ‹å™¨å‰å¾€ç«æ˜Ÿ  
   â‘¨ æ–°èƒ½æºè½¦ç”µæ± æŠ€æœ¯çªç ´ï¼Œç»­èˆªé‡Œç¨‹ç¿»å€  
   â‘© æŸæµè¡Œæ­Œæ‰‹æ–°ä¸“è¾‘24å°æ—¶é”€é‡ç ´çºªå½•
   ========================================
   ```

2. **é¦–æ¬¡è°ƒç”¨**ï¼š

   - DeepSeek-R1 åœ¨é¦–æ¬¡è°ƒç”¨æ—¶ç”Ÿæˆ Long-CoTï¼Œæ¨ç†æ—¶é—´è¾ƒé•¿ï¼ˆå› ä»»åŠ¡å¤æ‚åº¦è€Œå¼‚ï¼Œä½†æ˜¾è‘—é«˜äºåç»­è°ƒç”¨ï¼‰ã€‚

   - åŸå› ï¼šé¦–æ¬¡è°ƒç”¨éœ€å®Œæ•´åˆ†è§£é—®é¢˜å¹¶ç”Ÿæˆè¯¦ç»†æ¨ç†é“¾ã€‚

3. **åç»­ Function Calling**ï¼š

   - å·¥å…·è°ƒç”¨åï¼Œæ¨¡å‹åŸºäºå·²æœ‰ä¸Šä¸‹æ–‡å’Œç»“æœæ¨ç†ã€‚

   - Thinking å’Œ Reasoning æ—¶é—´æ˜¾è‘—ç¼©çŸ­ï¼Œä»…ä¸ºé¦–æ¬¡è°ƒç”¨çš„ 10%-20%ã€‚

   - åŸå› ï¼šåç»­è°ƒç”¨ä¾èµ–å·²æœ‰æ¨ç†åŸºç¡€ï¼Œå‡å°‘é‡å¤è®¡ç®—ã€‚

4. **æ•ˆç‡å½±å“**ï¼š

   - Long-CoT ä»…åœ¨é¦–æ¬¡è°ƒç”¨æ—¶æ˜¾è‘—ï¼Œåç»­ Function Calling çš„çŸ­æ¨ç†ä¸æ‹–ç´¯æ•´ä½“æ•ˆç‡ã€‚
   - å¤šæ¬¡æ¨¡å‹è°ƒç”¨è™½éå•æ¬¡ ReActï¼Œä½†è¿­ä»£æ•ˆç‡ä»é«˜ï¼Œæœªè§æ€§èƒ½æ˜æ˜¾ä¸‹é™ã€‚

#### ç»“è®ºä¸äº®ç‚¹

- **é«˜æ•ˆæ€§éªŒè¯**ï¼šLong-CoT é›†ä¸­äºé¦–æ¬¡è°ƒç”¨ï¼Œåç»­ Function Calling é«˜æ•ˆã€‚
- **ç°çŠ¶åæ€**ï¼šç›¸æ¯”äºåŸæœ‰çš„éreasoningç³»åˆ—æ¨¡å‹ï¼ŒDeepSeek-R1 çš„çŸ­æ¨ç†ç‰¹æ€§å¼¥è¡¥äº†æ•ˆæœçŸ­æ¿ã€‚
- **é€‚ç”¨æ€§**ï¼šé€‚åˆå¤æ‚å¤šè½®å·¥å…·è°ƒç”¨ä»»åŠ¡ï¼Œæ— éœ€æ‹…å¿ƒæ¨ç†æ—¶é•¿ç´¯ç§¯ã€‚
"""
    
    # åˆ›å»º MarkdownChunker å®ä¾‹
    chunker = MarkdownChunker(
        markdown_headers_to_split_on=[
            ("#", "h1"), 
            ("##", "h2"), 
            ("###", "h3"),
            ("####", "h4"),
            ("#####", "h5"),
            ("######", "h6")
        ],
        strip_headers=True
    )
    
    # è°ƒç”¨ chunk æ–¹æ³•
    documents = chunker.chunk(text=sample_text, title="æµ‹è¯•æ–‡æ¡£.md")
    
    # è¾“å‡ºç»“æœ
    print(f"æ–‡æ¡£æ•°é‡: {len(documents)}")
    for i, doc in enumerate(documents):
        print(f"chunk {i}: \n{doc.chunk}")
        print(f"metadata: {doc.metadata}")
        print(f"formatted chunk: \n{doc.format_chunk()}")
        print("-" * 50)